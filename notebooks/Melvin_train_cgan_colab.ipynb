{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import all packages and set paths and environment attributes"
      ],
      "metadata": {
        "id": "wD3_LvBJGf4j"
      },
      "id": "wD3_LvBJGf4j"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oRwsyME9a3_",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1761741302880,
          "user_tz": -120,
          "elapsed": 15,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "7c548dd0-8976-4ee8-80a3-7dfb6401ae6c"
      },
      "id": "6oRwsyME9a3_",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: google.colab.auth.authenticate_user() is not supported in Colab Enterprise.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mise à jour système\n",
        "!apt-get update\n",
        "!apt-get upgrade -y33\n",
        "\n",
        "# Installation Python 3.9\n",
        "!apt-get install -y python3.9 python3.9-dev python3.9-venv\n",
        "\n",
        "# Configuration\n",
        "!update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 1\n",
        "!update-alternatives --config python3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XiKtPv7CgnT",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1761742864692,
          "user_tz": -120,
          "elapsed": 72801,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "80185202-8af8-411e-b296-e5a89f1205d1"
      },
      "id": "1XiKtPv7CgnT",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.83)] [Connecting to security.\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r                                                                               \rHit:3 https://packages.cloud.google.com/apt gcsfuse-jammy InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.83)] [Connecting to security.\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: https://packages.cloud.google.com/apt/dists/gcsfuse-jammy/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Calculating upgrade... Done\n",
            "The following packages have been kept back:\n",
            "  libcudnn9-cuda-12 libcudnn9-dev-cuda-12 libnccl-dev libnccl2 libpam-systemd\n",
            "  libsystemd0 linux-headers-generic r-cran-ggplot2 r-cran-promises\n",
            "  r-cran-rmarkdown systemd systemd-sysv\n",
            "0 upgraded, 0 newly installed, 0 to remove and 12 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "python3.9 is already the newest version (3.9.24-1+jammy1).\n",
            "python3.9-dev is already the newest version (3.9.24-1+jammy1).\n",
            "python3.9-venv is already the newest version (3.9.24-1+jammy1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 12 not upgraded.\n",
            "There are 3 choices for the alternative python3 (providing /usr/bin/python3).\n",
            "\n",
            "  Selection    Path                 Priority   Status\n",
            "------------------------------------------------------------\n",
            "  0            /usr/bin/python3.11   2         auto mode\n",
            "  1            /usr/bin/python3.10   1         manual mode\n",
            "  2            /usr/bin/python3.11   2         manual mode\n",
            "* 3            /usr/bin/python3.9    1         manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "SW7wMmZ0Cbci",
        "executionInfo": {
          "status": "error",
          "timestamp": 1761742919459,
          "user_tz": -120,
          "elapsed": 11,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "c834fed1-284a-4d98-de81-b34926d44e6a"
      },
      "id": "SW7wMmZ0Cbci",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'python' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1576056767.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpython\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'python' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# import torch\n",
        "\n",
        "# # Vérification TensorFlow\n",
        "# print(\"TensorFlow GPU Check:\")\n",
        "# print(\"TensorFlow version:\", tf.__version__)\n",
        "# print(\"GPU disponibles:\", tf.config.list_physical_devices('GPU'))\n",
        "# print(\"GPU actif:\", tf.test.is_built_with_cuda())\n",
        "\n",
        "# # Vérification PyTorch\n",
        "# print(\"\\nPyTorch GPU Check:\")\n",
        "# print(\"Torch version:\", torch.__version__)\n",
        "# print(\"GPU disponible:\", torch.cuda.is_available())\n",
        "\n",
        "# if torch.cuda.is_available():\n",
        "#     print(\"Nombre de GPUs:\", torch.cuda.device_count())\n",
        "#     print(\"Nom du GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "# # Test de calcul GPU\n",
        "# print(\"\\nTest de calcul GPU:\")\n",
        "# try:\n",
        "#     with tf.device('/GPU:0'):\n",
        "#         a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
        "#         b = tf.constant([[5.0, 6.0], [7.0, 8.0]])\n",
        "#         c = tf.matmul(a, b)\n",
        "#     print(\"Calcul TensorFlow GPU réussi\")\n",
        "# except Exception as e:\n",
        "#     print(\"Erreur TensorFlow GPU:\", e)\n",
        "\n",
        "# try:\n",
        "#     x = torch.cuda.FloatTensor(2, 2)\n",
        "#     print(\"Calcul PyTorch GPU réussi\")\n",
        "# except Exception as e:\n",
        "#     print(\"Erreur PyTorch GPU:\", e)"
      ],
      "metadata": {
        "id": "wyjvzrz49wgi",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1761741914679,
          "user_tz": -120,
          "elapsed": 3,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "wyjvzrz49wgi",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vérification TensorFlow\n",
        "print(\"TensorFlow GPU Check:\")\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU disponibles:\", tf.config.list_physical_devices('GPU'))\n",
        "print(\"GPU actif:\", tf.test.is_built_with_cuda())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DDinUZr-KtX",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1761741420646,
          "user_tz": -120,
          "elapsed": 7,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "fa41f7a1-0cd1-47eb-8c36-3470eefbf871"
      },
      "id": "7DDinUZr-KtX",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow GPU Check:\n",
            "TensorFlow version: 2.19.0\n",
            "GPU disponibles: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "GPU actif: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/MELAI-1/downscaling-cgan.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkRg1b6GPWxP",
        "outputId": "e7f4c8b3-5449-4e9b-8535-aa2205269358",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1761741495243,
          "user_tz": -120,
          "elapsed": 4260,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "pkRg1b6GPWxP",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'downscaling-cgan'...\n",
            "remote: Enumerating objects: 3333, done.\u001b[K\n",
            "remote: Counting objects: 100% (506/506), done.\u001b[K\n",
            "remote: Compressing objects: 100% (172/172), done.\u001b[K\n",
            "remote: Total 3333 (delta 419), reused 359 (delta 334), pack-reused 2827 (from 3)\u001b[K\n",
            "Receiving objects: 100% (3333/3333), 83.83 MiB | 35.12 MiB/s, done.\n",
            "Resolving deltas: 100% (2370/2370), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copier tous les fichiers du répertoire\n",
        "!gsutil -m cp -r gs://melvin_aims_bucket/tfrecords/2018/ /content/"
      ],
      "metadata": {
        "id": "vsvVyLh6TXox",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1761741587430,
          "user_tz": -120,
          "elapsed": 2459,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "vsvVyLh6TXox",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7mJUN6A6AGtB"
      },
      "id": "7mJUN6A6AGtB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "import os\n",
        "\n",
        "# Authentification\n",
        "storage_client = storage.Client()\n",
        "\n",
        "# Sélection du bucket\n",
        "bucket = storage_client.bucket('melvin_aims_bucket')\n",
        "\n",
        "# Define the local target directory\n",
        "prefix = 'tfrecords/2018/'\n",
        "local_target_dir = f'/content/{prefix}' # This will be '/content/tfrecords/2018/'\n",
        "\n",
        "# Create the local directory if it does not exist\n",
        "os.makedirs(local_target_dir, exist_ok=True)\n",
        "print(f\"Ensured local directory exists: {local_target_dir}\")\n",
        "\n",
        "# Lister les fichiers avec le préfixe\n",
        "blobs = bucket.list_blobs(prefix=prefix)\n",
        "\n",
        "# Copier tous les fichiers\n",
        "for blob in blobs:\n",
        "    # Skip directory markers (blobs that end with / or are identical to the prefix)\n",
        "    if blob.name.endswith('/') or blob.name == prefix:\n",
        "        print(f\"Skipping directory marker: {blob.name}\")\n",
        "        continue\n",
        "\n",
        "    # Construct the full destination path for the file\n",
        "    destination_file_name = os.path.join(local_target_dir, os.path.basename(blob.name))\n",
        "\n",
        "    # Ensure the directory exists\n",
        "    os.makedirs(os.path.dirname(destination_file_name), exist_ok=True)\n",
        "\n",
        "    # Vérifier si c'est vraiment un fichier avant de télécharger\n",
        "    if not blob.name.endswith('/'):\n",
        "        blob.download_to_filename(destination_file_name)\n",
        "        print(f'Fichier {blob.name} téléchargé vers {destination_file_name}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1761741886192,
          "user_tz": -120,
          "elapsed": 20652,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "8605f102-cce3-4fd1-ed0d-17e58857fdea",
        "id": "ugbh_B_bAG1t"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensured local directory exists: /content/tfrecords/2018/\n",
            "Skipping directory marker: tfrecords/2018/\n",
            "Fichier tfrecords/2018/data_config.yaml téléchargé vers /content/tfrecords/2018/data_config.yaml\n",
            "Fichier tfrecords/2018/git_commit.txt téléchargé vers /content/tfrecords/2018/git_commit.txt\n",
            "Fichier tfrecords/2018/train_0.0.0.tfrecords téléchargé vers /content/tfrecords/2018/train_0.0.0.tfrecords\n",
            "Fichier tfrecords/2018/train_0.1.0.tfrecords téléchargé vers /content/tfrecords/2018/train_0.1.0.tfrecords\n",
            "Fichier tfrecords/2018/train_0.2.0.tfrecords téléchargé vers /content/tfrecords/2018/train_0.2.0.tfrecords\n",
            "Fichier tfrecords/2018/train_0.3.0.tfrecords téléchargé vers /content/tfrecords/2018/train_0.3.0.tfrecords\n",
            "Fichier tfrecords/2018/train_12.0.0.tfrecords téléchargé vers /content/tfrecords/2018/train_12.0.0.tfrecords\n",
            "Fichier tfrecords/2018/train_12.1.0.tfrecords téléchargé vers /content/tfrecords/2018/train_12.1.0.tfrecords\n",
            "Fichier tfrecords/2018/train_12.2.0.tfrecords téléchargé vers /content/tfrecords/2018/train_12.2.0.tfrecords\n",
            "Fichier tfrecords/2018/train_12.3.0.tfrecords téléchargé vers /content/tfrecords/2018/train_12.3.0.tfrecords\n",
            "Fichier tfrecords/2018/train_18.0.0.tfrecords téléchargé vers /content/tfrecords/2018/train_18.0.0.tfrecords\n",
            "Fichier tfrecords/2018/train_18.1.0.tfrecords téléchargé vers /content/tfrecords/2018/train_18.1.0.tfrecords\n",
            "Fichier tfrecords/2018/train_18.2.0.tfrecords téléchargé vers /content/tfrecords/2018/train_18.2.0.tfrecords\n",
            "Fichier tfrecords/2018/train_18.3.0.tfrecords téléchargé vers /content/tfrecords/2018/train_18.3.0.tfrecords\n",
            "Fichier tfrecords/2018/train_6.0.0.tfrecords téléchargé vers /content/tfrecords/2018/train_6.0.0.tfrecords\n",
            "Fichier tfrecords/2018/train_6.1.0.tfrecords téléchargé vers /content/tfrecords/2018/train_6.1.0.tfrecords\n",
            "Fichier tfrecords/2018/train_6.2.0.tfrecords téléchargé vers /content/tfrecords/2018/train_6.2.0.tfrecords\n",
            "Fichier tfrecords/2018/train_6.3.0.tfrecords téléchargé vers /content/tfrecords/2018/train_6.3.0.tfrecords\n"
          ]
        }
      ],
      "id": "ugbh_B_bAG1t"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cartopy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wE9pqDalJ_PF",
        "outputId": "abdd121d-cd23-4c97-cf6f-76b27d053512",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1761741945142,
          "user_tz": -120,
          "elapsed": 7406,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "wE9pqDalJ_PF",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cartopy in /usr/local/lib/python3.11/dist-packages (0.25.0)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from cartopy) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.11/dist-packages (from cartopy) (3.10.0)\n",
            "Requirement already satisfied: shapely>=2.0 in /usr/local/lib/python3.11/dist-packages (from cartopy) (2.1.1)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from cartopy) (25.0)\n",
            "Requirement already satisfied: pyshp>=2.3 in /usr/local/lib/python3.11/dist-packages (from cartopy) (2.3.1)\n",
            "Requirement already satisfied: pyproj>=3.3.1 in /usr/local/lib/python3.11/dist-packages (from cartopy) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (2.9.0.post0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from pyproj>=3.3.1->cartopy) (2025.8.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->cartopy) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6cj3_35OP_6u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "executionInfo": {
          "status": "error",
          "timestamp": 1761741947084,
          "user_tz": -120,
          "elapsed": 10,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "1d85ad03-702a-4455-901d-03765d9aa9ee"
      },
      "id": "6cj3_35OP_6u",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'sys' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2472750642.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#add downscaling to the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/content/downscaling-cgan\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'sys' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install netCDF4 xarray timezonefinder numpy pandas matplotlib scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwJXnJevQUYh",
        "outputId": "e849c008-650a-48fd-d3d9-5d767ac34f4f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1761741969311,
          "user_tz": -120,
          "elapsed": 6470,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "BwJXnJevQUYh",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting netCDF4\n",
            "  Downloading netcdf4-1.7.3-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.11/dist-packages (2025.7.1)\n",
            "Collecting timezonefinder\n",
            "  Downloading timezonefinder-8.1.0-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.16.1)\n",
            "Collecting cftime (from netCDF4)\n",
            "  Downloading cftime-1.6.5-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from netCDF4) (2025.8.3)\n",
            "Requirement already satisfied: packaging>=24.1 in /usr/local/lib/python3.11/dist-packages (from xarray) (25.0)\n",
            "Collecting h3>4 (from timezonefinder)\n",
            "  Downloading h3-4.3.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: cffi<3,>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from timezonefinder) (1.17.1)\n",
            "Requirement already satisfied: flatbuffers>=25.2.10 in /usr/local/lib/python3.11/dist-packages (from timezonefinder) (25.2.10)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi<3,>=1.15.1->timezonefinder) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading netcdf4-1.7.3-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading timezonefinder-8.1.0-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.2/28.2 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h3-4.3.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cftime-1.6.5-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m128.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h3, cftime, timezonefinder, netCDF4\n",
            "Successfully installed cftime-1.6.5 h3-4.3.1 netCDF4-1.7.3 timezonefinder-8.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install metpy properscoring"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4ybJ2JyRyBP",
        "outputId": "d7b94c25-b52e-4073-953d-bc64f7c1bcd5",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1761741975115,
          "user_tz": -120,
          "elapsed": 5808,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "q4ybJ2JyRyBP",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting metpy\n",
            "  Downloading metpy-1.7.1-py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting properscoring\n",
            "  Downloading properscoring-0.1-py2.py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: matplotlib>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from metpy) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from metpy) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from metpy) (2.2.2)\n",
            "Collecting pint>=0.17 (from metpy)\n",
            "  Downloading pint-0.25-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pooch>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from metpy) (1.8.2)\n",
            "Requirement already satisfied: pyproj>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from metpy) (3.7.1)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from metpy) (1.16.1)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from metpy) (5.7.1)\n",
            "Requirement already satisfied: xarray>=2022.6.0 in /usr/local/lib/python3.11/dist-packages (from metpy) (2025.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->metpy) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->metpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->metpy) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->metpy) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->metpy) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->metpy) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->metpy) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->metpy) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->metpy) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.0->metpy) (2025.2)\n",
            "Collecting flexcache>=0.3 (from pint>=0.17->metpy)\n",
            "  Downloading flexcache-0.3-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting flexparser>=0.4 (from pint>=0.17->metpy)\n",
            "  Downloading flexparser-0.4-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: platformdirs>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pint>=0.17->metpy) (4.3.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from pint>=0.17->metpy) (4.14.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.2.0->metpy) (2.32.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from pyproj>=3.3.0->metpy) (2025.8.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.5.0->metpy) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.2.0->metpy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.2.0->metpy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.2.0->metpy) (2.5.0)\n",
            "Downloading metpy-1.7.1-py3-none-any.whl (424 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m424.4/424.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading properscoring-0.1-py2.py3-none-any.whl (23 kB)\n",
            "Downloading pint-0.25-py3-none-any.whl (305 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.5/305.5 kB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flexcache-0.3-py3-none-any.whl (13 kB)\n",
            "Downloading flexparser-0.4-py3-none-any.whl (27 kB)\n",
            "Installing collected packages: flexparser, flexcache, properscoring, pint, metpy\n",
            "Successfully installed flexcache-0.3 flexparser-0.4 metpy-1.7.1 pint-0.25 properscoring-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xesmf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65DCROrhAXOM",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1761742014719,
          "user_tz": -120,
          "elapsed": 5325,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "cc967dec-7395-4bf9-b3ac-bc6d8412f31b"
      },
      "id": "65DCROrhAXOM",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xesmf\n",
            "  Downloading xesmf-0.8.10-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting cf-xarray>=0.5.1 (from xesmf)\n",
            "  Downloading cf_xarray-0.10.9-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numba>=0.55.2 in /usr/local/lib/python3.11/dist-packages (from xesmf) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.11/dist-packages (from xesmf) (2.0.2)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.11/dist-packages (from xesmf) (2.1.1)\n",
            "Collecting sparse>=0.8.0 (from xesmf)\n",
            "  Downloading sparse-0.17.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: xarray>=0.16.2 in /usr/local/lib/python3.11/dist-packages (from xesmf) (2025.7.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.55.2->xesmf) (0.43.0)\n",
            "Requirement already satisfied: packaging>=24.1 in /usr/local/lib/python3.11/dist-packages (from xarray>=0.16.2->xesmf) (25.0)\n",
            "Requirement already satisfied: pandas>=2.2 in /usr/local/lib/python3.11/dist-packages (from xarray>=0.16.2->xesmf) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2->xarray>=0.16.2->xesmf) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2->xarray>=0.16.2->xesmf) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2->xarray>=0.16.2->xesmf) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2->xarray>=0.16.2->xesmf) (1.17.0)\n",
            "Downloading xesmf-0.8.10-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cf_xarray-0.10.9-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sparse-0.17.0-py2.py3-none-any.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.4/259.4 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sparse, cf-xarray, xesmf\n",
            "Successfully installed cf-xarray-0.10.9 sparse-0.17.0 xesmf-0.8.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install ESMF\n",
        "\n"
      ],
      "metadata": {
        "id": "x_33a81xAggS",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1761742209495,
          "user_tz": -120,
          "elapsed": 10,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "x_33a81xAggS",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataframe:\n",
        "# uuid: 4F8CEDB1-6C1B-4D51-8EE3-EAB24F87ABE9\n",
        "# output_variable:\n",
        "\n",
        "import google.colabsqlviz.explore_dataframe as _vizcell\n",
        "_vizcell.explore_dataframe(df_or_df_name='', uuid='4F8CEDB1-6C1B-4D51-8EE3-EAB24F87ABE9')"
      ],
      "metadata": {
        "colab_type": "viz",
        "id": "fuCLw3d5Baih"
      },
      "id": "fuCLw3d5Baih",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Se placer dans le bon répertoire AVANT d'importer\n",
        "os.chdir('/content/downscaling-cgan')\n",
        "\n",
        "# Ajouter au path\n",
        "sys.path.insert(0, '/content/downscaling-cgan')\n",
        "sys.path.insert(0, '/content')\n",
        "\n",
        "# Maintenant importer\n",
        "from dsrnngan.data.setupdata import setup_data\n",
        "#add downscaling to the path\n",
        "sys.path.insert(1,\"/content/downscaling-cgan\")"
      ],
      "metadata": {
        "id": "I_9ED5SaRp-v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "executionInfo": {
          "status": "error",
          "timestamp": 1761742216595,
          "user_tz": -120,
          "elapsed": 14,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "16f57dce-95bd-46e0-9429-8e40a5340d47"
      },
      "id": "I_9ED5SaRp-v",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after 'except' statement on line 9 (util.py, line 12)",
          "traceback": [
            "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3553\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \u001b[1;32m\"/tmp/ipython-input-3264025949.py\"\u001b[0m, line \u001b[1;32m12\u001b[0m, in \u001b[1;35m<cell line: 0>\u001b[0m\n    from dsrnngan.data.setupdata import setup_data\n",
            "  File \u001b[1;32m\"/content/downscaling-cgan/dsrnngan/data/setupdata.py\"\u001b[0m, line \u001b[1;32m11\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from dsrnngan.data import tfrecords_generator, setupdata\n",
            "  File \u001b[1;32m\"/content/downscaling-cgan/dsrnngan/data/tfrecords_generator.py\"\u001b[0m, line \u001b[1;32m13\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from dsrnngan.data.data import file_exists, denormalise\n",
            "  File \u001b[1;32m\"/content/downscaling-cgan/dsrnngan/data/data.py\"\u001b[0m, line \u001b[1;32m17\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    import xesmf as xe\n",
            "\u001b[0;36m  File \u001b[0;32m\"/usr/local/lib/python3.11/dist-packages/xesmf/__init__.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from . import data, util\u001b[0m\n",
            "\u001b[0;36m  File \u001b[0;32m\"/usr/local/lib/python3.11/dist-packages/xesmf/util.py\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    LON_CF_ATTRS = {'standard_name': 'longitude', 'units': 'degrees_east'}\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'except' statement on line 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import json\n",
        "import cartopy\n",
        "\n",
        "\n",
        "from tensorflow import config as tf_config\n",
        "import sys\n",
        "import joblib\n",
        "\n",
        "\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import cartopy.crs as ccrs\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from dsrnngan.data import setupdata\n",
        "from dsrnngan.data.setupdata import setup_full_image_dataset\n",
        "from dsrnngan.model import setupmodel\n",
        "from dsrnngan.model.train import train_model\n",
        "from dsrnngan.data.data import all_ngcm_fields\n",
        "from dsrnngan.evaluation.evaluation import evaluate_multiple_checkpoints\n",
        "\n"
      ],
      "metadata": {
        "id": "2Ek9hjTx9ZC3"
      },
      "id": "2Ek9hjTx9ZC3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dsrnngan.data.setupdata import setup_data"
      ],
      "metadata": {
        "id": "Ob4iSXVFRUgT"
      },
      "id": "Ob4iSXVFRUgT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check whether GPU is available or not"
      ],
      "metadata": {
        "id": "ihuulqhYGygu"
      },
      "id": "ihuulqhYGygu"
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_devices = tf_config.list_physical_devices('GPU')\n",
        "\n",
        "print(gpu_devices)\n",
        "\n",
        "if len(gpu_devices) == 0:\n",
        "    print('GPU devices are not being seen')"
      ],
      "metadata": {
        "id": "cWUpPnDEGyBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32af61a8-e286-412a-a1b0-d00f13af45aa"
      },
      "id": "cWUpPnDEGyBv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "GPU devices are not being seen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load in configuration files for this you need to specify the path to the configuration file to use, typically this will be config/config.yaml\n",
        "\n",
        "## Please refer to the README under [model/](https://github.com/snath-xoc/cGAN_tutorial/tree/main/model) for information. Most importantly the model configuration will specify:\n",
        "### 1) mode: between GAN, VAEGAN and det <br> 2) arch: between forceconv, forceconv-long and normal <br> 3) filters_gen and filters_disc: width of generator and discriminator network <br> 4) lr_gen and lr_disc: learning rate for generator and discriminator <br> 5) train_years and val_years: training and validation years to use <br> 6) num_samples: no. of samples to train on <br> 7) steps_per_checkpoint: number of batches per checkpoint save <br> 8) batch_size: size of batches <br> 9) CLtype: between CRPS, CRPS_phys, ensmeanMSE, ensmeanMSE_phys <br> 10) content_loss_weight: how much to weight content loss in loss calculation <br> 11) training_weights: training weights to assign when sampling each bin used in tfrecord creation"
      ],
      "metadata": {
        "id": "Druo_RgLaPWq"
      },
      "id": "Druo_RgLaPWq"
    },
    {
      "cell_type": "code",
      "source": [
        "# config_path=\"/home/melvin_aims_ac_za/downscaling-cgan/config/model_config.yaml\"\n",
        "config_path=\"/content/downscaling-cgan/config/model_config.yaml\""
      ],
      "metadata": {
        "id": "ROtUvPXGSMCd"
      },
      "id": "ROtUvPXGSMCd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to config\n",
        "config_path = config_path\n",
        "\n",
        "with open(config_path, \"r\") as f:\n",
        "    try:\n",
        "        setup_params = yaml.safe_load(f)\n",
        "        # print(setup_params)\n",
        "    except yaml.YAMLError as exc:\n",
        "        print(exc)\n",
        "\n",
        "mode = setup_params[\"mode\"]\n",
        "arch = setup_params[\"architecture\"]\n",
        "padding = setup_params[\"padding\"]\n",
        "log_folder = setup_params[\"log_folder\"]\n",
        "problem_type = setup_params[\"problem_type\"]\n",
        "filters_gen = setup_params[\"generator\"][\"filters_gen\"]\n",
        "lr_gen = setup_params[\"generator\"][\"learning_rate_gen\"]\n",
        "noise_channels = setup_params[\"generator\"][\"noise_channels\"]\n",
        "latent_variables = setup_params[\"generator\"][\"latent_variables\"]\n",
        "filters_disc = setup_params[\"discriminator\"][\"filters_disc\"]\n",
        "lr_disc = setup_params[\"discriminator\"][\"learning_rate_disc\"]\n",
        "train_years = setup_params[\"train\"][\"training_range\"]\n",
        "training_weights = setup_params[\"train\"][\"training_weights\"]\n",
        "num_samples = 3200\n",
        "steps_per_checkpoint = setup_params[\"train\"][\"steps_per_checkpoint\"]\n",
        "batch_size = setup_params[\"train\"][\"batch_size\"]\n",
        "kl_weight = setup_params[\"train\"][\"kl_weight\"]\n",
        "ensemble_size = setup_params[\"train\"][\"ensemble_size\"]\n",
        "CLtype = setup_params[\"train\"][\"CL_type\"]\n",
        "content_loss_weight = setup_params[\"train\"][\"content_loss_weight\"]\n",
        "val_years = setup_params[\"val\"][\"val_range\"]\n",
        "val_size = setup_params[\"val\"][\"val_size\"]\n",
        "num_images = setup_params[\"EVAL\"][\"num_batches\"]\n",
        "add_noise = setup_params[\"EVAL\"][\"add_postprocessing_noise\"]\n",
        "noise_factor = setup_params[\"EVAL\"][\"postprocessing_noise_factor\"]\n",
        "max_pooling = setup_params[\"EVAL\"][\"max_pooling\"]\n",
        "avg_pooling = setup_params[\"EVAL\"][\"avg_pooling\"]\n",
        "constant_fields = 2  # future todo: have dataset config file?\n",
        "\n",
        "# otherwise these are of type string, e.g. '1e-5'\n",
        "lr_gen = float(lr_gen)\n",
        "lr_disc = float(lr_disc)\n",
        "kl_weight = float(kl_weight)\n",
        "# noise_factor = float(noise_factor)\n",
        "content_loss_weight = float(content_loss_weight)\n",
        "\n",
        "print(\"Loaded in the configuration for model training from:\", config_path)\n",
        "print(\"\\n Model details are as follows,\\n mode:\",mode, \"\\n arch:\", arch, \"\\n filters_gen:\", filters_gen, \"\\n filters_disc:\", filters_disc)\n",
        "print(\"\\n Training details are as follows,\\n lr_gen:\", lr_gen, \"\\n lr_disc:\", lr_disc, \"\\n train_years:\", train_years, \"\\n val_years:\", val_years,\n",
        "     \"\\n CLtype:\", CLtype, \"\\n content_loss_weight:\", content_loss_weight)\n",
        "print(\"\\n During training the following protocol will be used, \\n num_samples:\", num_samples, \"\\n steps_per_checkpoint:\", steps_per_checkpoint,\n",
        "      \"\\n batch_size:\", batch_size, \"\\n training_weights:\", training_weights)\n"
      ],
      "metadata": {
        "id": "7AP_JeMVZ8Cs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53acc71a-6da1-49f6-f894-d998c660d7c8"
      },
      "id": "7AP_JeMVZ8Cs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded in the configuration for model training from: /content/downscaling-cgan/config/model_config.yaml\n",
            "\n",
            " Model details are as follows,\n",
            " mode: GAN \n",
            " arch: forceconv \n",
            " filters_gen: 128 \n",
            " filters_disc: 256\n",
            "\n",
            " Training details are as follows,\n",
            " lr_gen: 1e-05 \n",
            " lr_disc: 1e-05 \n",
            " train_years: ['201801', '201802'] \n",
            " val_years: ['201806', '201905'] \n",
            " CLtype: ensmeanMSE \n",
            " content_loss_weight: 1000.0\n",
            "\n",
            " During training the following protocol will be used, \n",
            " num_samples: 3200 \n",
            " steps_per_checkpoint: 3200 \n",
            " batch_size: 2 \n",
            " training_weights: [0.25, 0.25, 0.25, 0.25]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up data by calling [setupdata.setup_data](https://github.com/snath-xoc/cGAN_tutorial/blob/main/setupdata.py#L42), this will return:\n",
        "### 1) batch_gen_train: training dataset created by sampling from tfrecords according to the training weights <br> 2) data_gen_valid: validation dataset of full images to validate over the whole domain"
      ],
      "metadata": {
        "id": "dGzpJ0DSaTYA"
      },
      "id": "dGzpJ0DSaTYA"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from dsrnngan.data.data import load_fcst_radar_batch, load_hires_constants, all_fcst_hours, DATA_PATHS, all_ifs_fields, all_era5_fields, input_fields\n",
        "##🚩import ngcm function\n",
        "from dsrnngan.data.data import load_ngcm, all_ngcm_fields,  get_ngcm_stats, all_ngcm_fields\n",
        "from dsrnngan.utils.read_config import read_model_config, get_data_paths, get_lat_lon_range_from_config,read_data_config\n"
      ],
      "metadata": {
        "id": "gpszcd30FClw"
      },
      "id": "gpszcd30FClw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_config=read_model_config()\n",
        "print(\"************model config**************\")\n",
        "model_config\n",
        "data_config=read_data_config()\n",
        "print(\"***********data config***************\")\n",
        "data_config"
      ],
      "metadata": {
        "id": "EBzNvFIAEhzh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29ab975e-7582-4d73-ba65-13ca7ff1b8cc"
      },
      "id": "EBzNvFIAEhzh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************model config**************\n",
            "***********data config***************\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "namespace(data_paths='BLUE_PEBBLE',\n",
              "          fcst_data_source='ngcm',\n",
              "          obs_data_source='imerg',\n",
              "          normalisation_year=2018,\n",
              "          num_samples=320000,\n",
              "          normalise_inputs=True,\n",
              "          output_normalisation='log',\n",
              "          num_classes=4,\n",
              "          downscaling_factor=1,\n",
              "          min_latitude=-18.14,\n",
              "          max_latitude=29,\n",
              "          latitude_step_size=2.8,\n",
              "          min_longitude=16,\n",
              "          max_longitude=60,\n",
              "          input_image_shape='(384, 352, 11)',\n",
              "          constants_image_shape='(384, 352, 2)',\n",
              "          output_image_shape='(384, 352, 1)',\n",
              "          seed=42,\n",
              "          longitude_step_size=2.8,\n",
              "          input_fields=['evaporation',\n",
              "                        'precipitation_cumulative_mean',\n",
              "                        'specific_cloud_ice_water_content_500',\n",
              "                        'specific_cloud_ice_water_content_700',\n",
              "                        'specific_cloud_ice_water_content_850',\n",
              "                        'u_component_of_wind_500',\n",
              "                        'u_component_of_wind_700',\n",
              "                        'u_component_of_wind_850',\n",
              "                        'v_component_of_wind_500',\n",
              "                        'v_component_of_wind_700',\n",
              "                        'v_component_of_wind_850'],\n",
              "          constant_fields=['orography', 'lsm'],\n",
              "          input_normalisation_strategy={'tpq': {'negative_vals': False,\n",
              "                                         'normalisation': 'log'},\n",
              "                                        'tp': {'negative_vals': False,\n",
              "                                         'normalisation': 'log'},\n",
              "                                        'cp': {'negative_vals': False,\n",
              "                                         'normalisation': 'log'},\n",
              "                                        'pr': {'negative_vals': False,\n",
              "                                         'normalisation': 'log'},\n",
              "                                        'prl': {'negative_vals': False,\n",
              "                                         'normalisation': 'log'},\n",
              "                                        'prc': {'negative_vals': False,\n",
              "                                         'normalisation': 'log'},\n",
              "                                        'sp': {'normalisation': 'minmax'},\n",
              "                                        'u': {'normalisation': 'max'},\n",
              "                                        'v': {'normalisation': 'max'},\n",
              "                                        'w': {'normalisation': 'max'},\n",
              "                                        'r': {'normalisation': 'max'},\n",
              "                                        '2t': {'normalisation': 'minmax',\n",
              "                                         'negative_vals': False},\n",
              "                                        'cape': {'normalisation': 'log'},\n",
              "                                        'cin': {'normalisation': 'max'},\n",
              "                                        't': {'normalisation': 'minmax',\n",
              "                                         'negative_vals': False},\n",
              "                                        'tclw': {'normalisation': 'log'},\n",
              "                                        'tcwv': {'normalisation': 'max'},\n",
              "                                        'tisr': {'normalisation': 'max'}},\n",
              "          ngcm_input_normalisation_strategy={'evaporation': {'normalisation': 'log'},\n",
              "                                             'precipitation_cumulative_mean': {'negative_vals': False,\n",
              "                                              'normalisation': 'log'},\n",
              "                                             'specific_cloud_ice_water_content_500': {'negative_vals': False,\n",
              "                                              'normalisation': 'log'},\n",
              "                                             'specific_cloud_ice_water_content_700': {'negative_vals': False,\n",
              "                                              'normalisation': 'log'},\n",
              "                                             'specific_cloud_ice_water_content_850': {'negative_vals': False,\n",
              "                                              'normalisation': 'log'},\n",
              "                                             'u_component_of_wind_500': {'normalisation': 'minmax'},\n",
              "                                             'u_component_of_wind_700': {'normalisation': 'minmax'},\n",
              "                                             'u_component_of_wind_850': {'normalisation': 'minmax'},\n",
              "                                             'v_component_of_wind_500': {'normalisation': 'minmax'},\n",
              "                                             'v_component_of_wind_700': {'normalisation': 'minmax'},\n",
              "                                             'v_component_of_wind_850': {'normalisation': 'minmax'}},\n",
              "          paths={'BLUE_PEBBLE': {'GENERAL': {'IMERG': '/home/melvin_aims_ac_za/data/IMERG',\n",
              "                   'ERA5': '/bp1/geog-tropical/data/ERA-5/day',\n",
              "                   'IFS': '/bp1/geog-tropical/users/uz22147/east_africa_data/IFS',\n",
              "                   'OROGRAPHY': '/home/melvin_aims_ac_za/data/constants/elev.nc',\n",
              "                   'LSM': '/home/melvin_aims_ac_za/data/constants/lsm.nc',\n",
              "                   'LAKES': '/bp1/geog-tropical/users/uz22147/east_africa_data/constants/lake_mask.nc',\n",
              "                   'SEA': '/bp1/geog-tropical/users/uz22147/east_africa_data/constants/sea_mask.nc',\n",
              "                   'CONSTANTS': '/home/melvin_aims_ac_za/data/constants/',\n",
              "                   'NGCM': '/home/melvin_aims_ac_za/data/NGCM/',\n",
              "                   'STATS': '/home/melvin_aims_ac_za/data/constants/neuralgcm_Horn_Africa_2018_stats.pkl'},\n",
              "                  'NGCM': {'evaporation': [],\n",
              "                   'precipitation_cumulative_mean': [],\n",
              "                   'specific_cloud_ice_water_content_500': [],\n",
              "                   'specific_cloud_ice_water_content_700': [],\n",
              "                   'specific_cloud_ice_water_content_850': [],\n",
              "                   'u_component_of_wind_500': [],\n",
              "                   'u_component_of_wind_700': [],\n",
              "                   'u_component_of_wind_850': [],\n",
              "                   'v_component_of_wind_500': [],\n",
              "                   'v_component_of_wind_700': [],\n",
              "                   'v_component_of_wind_850': []},\n",
              "                  'TFRecords': {'tfrecords_path': '/home/melvin_aims_ac_za/data/tfrecords/final_tfrecord'}}},\n",
              "          load_constants=True,\n",
              "          input_channels=11,\n",
              "          class_bin_boundaries=None,\n",
              "          input_image_height=17,\n",
              "          input_image_width=16)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "records_folder =  data_config.paths['BLUE_PEBBLE']['TFRecords']['tfrecords_path']\n",
        "print(f\"the record folder is  : {records_folder}\")\n",
        "\n",
        "input_image_shape = (384, 352, 11)\n",
        "# input_image_shape=data_config.input_image_shape\n",
        "# print(f\"input_image_shape is  : {input_image_shape}\")\n",
        "\n",
        "constants_image_shape = (384, 352, 2)\n",
        "# constants_image_shape=data_config.input_image_shape\n",
        "# print(f\"constants_image_shape  is  : {constants_image_shape}\")\n",
        "\n",
        "output_image_shape = (384, 352, 1)\n",
        "# output_image_shape = data_config.output_image_shape\n",
        "# print(f\"output_image_shape is  : {output_image_shape}\")\n",
        "\n",
        "# random seed\n",
        "seed = data_config.seed\n",
        "# print(f\"random seed is  : {seed}\")\n"
      ],
      "metadata": {
        "id": "uM5KOYIFds9N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdf79305-a580-4808-eed1-279724ce601d"
      },
      "id": "uM5KOYIFds9N",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the record folder is  : /home/melvin_aims_ac_za/data/tfrecords/final_tfrecord\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OphDJVOsSwpi"
      },
      "id": "OphDJVOsSwpi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_gen_train, batch_gen_valid = setupdata.setup_data(\n",
        "    data_config=data_config,\n",
        "    model_config=model_config,\n",
        "    records_folder=records_folder,\n",
        "    fcst_shape=input_image_shape,\n",
        "    con_shape=constants_image_shape,\n",
        "    out_shape=output_image_shape,\n",
        "    weights=training_weights,\n",
        "    load_full_image=False,\n",
        "    seed=seed,\n",
        "    hour=[0,6,12,18])"
      ],
      "metadata": {
        "id": "gChcWDGpitDu"
      },
      "id": "gChcWDGpitDu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- VÉRIFICATION DU GÉNÉRATEUR DE VALIDATION ---\")\n",
        "\n",
        "# 1. Créez un itérateur\n",
        "valid_iterator = iter(batch_gen_valid)\n",
        "\n",
        "# 2. Extrayez le premier batch\n",
        "try:\n",
        "    fcst_batch_val, out_batch_val = next(valid_iterator)\n",
        "\n",
        "    # 3. Vérifiez les dimensions\n",
        "    print(\"Shape du Forecast (Validation) :\", fcst_batch_val)\n",
        "    print(\"Shape de l'Output (Validation) :\", out_batch_val)\n",
        "    print(\"tout se passe bien \")\n",
        "\n",
        "except StopIteration:\n",
        "    # Pour le validation/test, c'est normal si on extrait plus d'éléments qu'il n'y en a.\n",
        "    # Mais le premier appel à next() doit fonctionner.\n",
        "    print(\"❌ Échec : Le générateur de validation est vide ou ne peut pas produire de batch.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Échec : Erreur lors de l'inspection du générateur de validation. Erreur: {e}\")"
      ],
      "metadata": {
        "id": "epY2ekmYmxIk"
      },
      "id": "epY2ekmYmxIk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_gen_train, data_gen_valid = setupdata.setup_data(data_config=data_config, model_config=model_config, records_folder=records_folder, fcst_shape=input_image_shape, con_shape=constants_image_shape, out_shape=output_image_shape, weights=training_weights, load_full_image=False, seed=seed)"
      ],
      "metadata": {
        "id": "mhUZFFptDBg-"
      },
      "id": "mhUZFFptDBg-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Once again check output of validation dataset"
      ],
      "metadata": {
        "id": "NGOJJ5_LaX5o"
      },
      "id": "NGOJJ5_LaX5o"
    },
    {
      "cell_type": "code",
      "source": [
        "sample = batch_gen_valid.__getitem__(0)\n",
        "ax = plt.axes(projection = ccrs.PlateCarree())\n",
        "\n",
        "lats = np.arange(-13.65,24.65+0.1,0.1)\n",
        "lons = np.arange(19.15,54.25+0.1,0.1)\n",
        "\n",
        "mesh = ax.pcolormesh(lons, lats, sample[0]['hi_res_inputs'][0,:,:,0], cmap='terrain_r')\n",
        "plt.colorbar(mesh)\n",
        "plt.title('elevation')\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "ax = plt.axes(projection = ccrs.PlateCarree())\n",
        "\n",
        "mesh = ax.pcolormesh(lons, lats, sample[0]['hi_res_inputs'][0,:,:,1], cmap='Blues')\n",
        "plt.colorbar(mesh)\n",
        "plt.title('land-sea mask')\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "idx =  all_ngcm_fields.index(\"evaporation\")\n",
        "ax = plt.axes(projection = ccrs.PlateCarree())\n",
        "\n",
        "lats = np.arange(-13.65,24.65+0.1,0.1)\n",
        "lons = np.arange(19.15,54.25+0.1,0.1)\n",
        "\n",
        "mesh = ax.pcolormesh(lons, lats, sample[0]['lo_res_inputs'][0,:,:,idx], cmap='Reds')\n",
        "plt.colorbar(mesh)\n",
        "plt.title(f\"evaporation\")\n",
        "plt.savefig('evaporation_train.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "plt.close()\n"
      ],
      "metadata": {
        "id": "0DmED4aGaaYt"
      },
      "id": "0DmED4aGaaYt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now set up model for training, before doing so we also need to:\n",
        "### 1) Specify the downscaling factor with which the model will work. This is set in the [config/downscaling_factor.yaml](https://github.com/snath-xoc/cGAN_tutorial/blob/main/config/downscaling_factor.yaml) and is loaded in by calling ```read_downscaling_factor```<br> 2) Calculate the number of input channels which will be number of ```all_fcst_fields*4``` (see [create_tfrecords.ipynb](https://github.com/snath-xoc/cGAN_tutorial/blob/main/example_notebooks/create_tfrecords.ipynb) for more explanation)"
      ],
      "metadata": {
        "id": "T2eZkhYpaed6"
      },
      "id": "T2eZkhYpaed6"
    },
    {
      "cell_type": "code",
      "source": [
        "records_folder =  data_config.paths['BLUE_PEBBLE']['TFRecords']['tfrecords_path']\n",
        "print(f\"the record folder is  : {records_folder}\")\n",
        "\n",
        "# input_image_shape = (384, 352, 11)\n",
        "input_image_shape=data_config.input_image_shape\n",
        "print(f\"input_image_shape is  : {input_image_shape}\")\n",
        "\n",
        "# constants_image_shape = (384, 352, 2)\n",
        "constants_image_shape=data_config.input_image_shape\n",
        "print(f\"constants_image_shape  is  : {constants_image_shape}\")\n",
        "\n",
        "# output_image_shape = (384, 352, 1)\n",
        "output_image_shape = data_config.output_image_shape\n",
        "print(f\"output_image_shape is  : {output_image_shape}\")\n",
        "\n",
        "# random seed\n",
        "seed = data_config.seed\n",
        "print(f\"random seed is  : {seed}\")\n"
      ],
      "metadata": {
        "id": "0ROJ-f4nCU2C"
      },
      "id": "0ROJ-f4nCU2C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Downscaling factor to be used is:\", setup_params[\"downscaling_factor\"], \"with steps:\", setup_params[\"downscaling_steps\"])\n",
        "\n",
        "input_channels = 4 * len(all_ngcm_fields)\n",
        "\n",
        "print(\"Number of input channels are:\", input_channels)\n"
      ],
      "metadata": {
        "id": "l4CAsFLatph5"
      },
      "id": "l4CAsFLatph5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_config=read_model_config()\n",
        "print(\"************model config**************\")\n",
        "model_config\n",
        "data_config=read_data_config()\n",
        "print(\"***********data config***************\")\n",
        "data_config"
      ],
      "metadata": {
        "id": "CIR7zOaLCb02"
      },
      "id": "CIR7zOaLCb02",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
        "print(\"Setting up model\")\n",
        "\n",
        "model = setupmodel.setup_model(\n",
        "    model_config=model_config,\n",
        "    data_config=data_config\n",
        ")"
      ],
      "metadata": {
        "id": "AH1OH5K4wcJi"
      },
      "id": "AH1OH5K4wcJi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now try a first training of the model, for simplicity we hard code the num_samples, steps_per_checkpoint and batch_size as this is a dry run. Once done you can check the progress plots by navigating into the folder where log results are stored (printed below)"
      ],
      "metadata": {
        "id": "zdzIbhU3al5J"
      },
      "id": "zdzIbhU3al5J"
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from dsrnngan.model.train import train_model\n",
        "\n",
        "# Configuration\n",
        "num_samples = 6\n",
        "steps_per_checkpoint = 3\n",
        "batch_size = 2\n",
        "\n",
        "# Calculer le nombre de checkpoints nécessaires\n",
        "# Si train_model traite steps_per_checkpoint échantillons (pas steps*batch):\n",
        "num_checkpoints = (num_samples + steps_per_checkpoint - 1) // steps_per_checkpoint\n",
        "# = (6 + 3 - 1) // 3 = 8 // 3 = 2 checkpoints\n",
        "\n",
        "# OU si train_model traite steps_per_checkpoint * batch_size:\n",
        "# num_checkpoints = (num_samples + (steps_per_checkpoint * batch_size) - 1) // (steps_per_checkpoint * batch_size)\n",
        "# = (6 + 6 - 1) // 6 = 11 // 6 = 1 checkpoint\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  - Total samples: {num_samples}\")\n",
        "print(f\"  - Steps per checkpoint: {steps_per_checkpoint}\")\n",
        "print(f\"  - Batch size: {batch_size}\")\n",
        "print(f\"  - Nombre de checkpoints: {num_checkpoints}\")\n",
        "print()\n",
        "\n",
        "print(f\"Training across {num_checkpoints} checkpoints\")\n",
        "print(f\"Logs in: {log_folder}\")\n",
        "\n",
        "if not os.path.exists(log_folder+\"models/\"):\n",
        "    os.makedirs(log_folder+\"models/\")\n",
        "\n",
        "model_weights_root = os.path.join(log_folder, \"models\")\n",
        "\n",
        "# Boucle simplifiée: itérer directement sur les checkpoints\n",
        "for checkpoint in tqdm(range(1, num_checkpoints + 1), desc=\"Training\"):\n",
        "    gc.collect()\n",
        "\n",
        "    print(f\"\\n--- Checkpoint {checkpoint}/{num_checkpoints} ---\")\n",
        "\n",
        "    # Entraînement\n",
        "    loss_log = train_model(\n",
        "        model=model,\n",
        "        mode=mode,\n",
        "        batch_gen_train=batch_gen_train,\n",
        "        data_gen_valid=batch_gen_valid,\n",
        "        noise_channels=noise_channels,\n",
        "        latent_variables=latent_variables,\n",
        "        checkpoint=checkpoint,\n",
        "        steps_per_checkpoint=steps_per_checkpoint,\n",
        "        num_cases=val_size,\n",
        "        plot_fn=os.path.join(log_folder, \"progress\"),\n",
        "        log_wandb=False,\n",
        "    )\n",
        "\n",
        "    # Calculer le nombre d'échantillons traités jusqu'à maintenant\n",
        "    training_samples = checkpoint * steps_per_checkpoint  # Ajustez selon votre cas\n",
        "\n",
        "    # Sauvegarde des poids\n",
        "    gen_weights_file = os.path.join(\n",
        "        model_weights_root, f\"gen_weights-{training_samples:07d}.h5\"\n",
        "    )\n",
        "    model.gen.save_weights(gen_weights_file)\n",
        "\n",
        "    print(f\"✅ Checkpoint {checkpoint} terminé ({training_samples} samples)\")\n",
        "\n",
        "print(\"\\n🎉 Training terminé!\")"
      ],
      "metadata": {
        "id": "ZLpkONCcAmWK"
      },
      "id": "ZLpkONCcAmWK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dsrnngan.model.train import train_model\n",
        ",\n",
        "\n",
        "num_samples = 6\n",
        "steps_per_checkpoint = 3\n",
        "batch_size = 2\n",
        "num_checkpoints = int(num_samples / (steps_per_checkpoint * batch_size))\n",
        "checkpoint = 1\n",
        "\n",
        "print(\"Doing a training across\", num_checkpoints,\"checkpoints, all log results will be stored in,\", log_folder)\n",
        "if not os.path.exists(log_folder+\"models/\"):\n",
        "    os.makedirs(log_folder+\"models/\")\n",
        "\n",
        "model_weights_root = os.path.join(log_folder, \"models\")\n",
        "print(model_weights_root )\n",
        "\n",
        "training_samples = 0\n",
        "\n",
        "while training_samples < num_samples:  # main training loop\n",
        "    gc.collect()\n",
        "    print(f\"Checkpoint {checkpoint}/{num_checkpoints}\")\n",
        "\n",
        "    # train for some number of batches\n",
        "    loss_log = train_model(\n",
        "        model=model,\n",
        "        mode=mode,\n",
        "        batch_gen_train=batch_gen_train,\n",
        "        data_gen_valid=batch_gen_valid ,\n",
        "        noise_channels=noise_channels,\n",
        "        latent_variables=latent_variables,\n",
        "        checkpoint=checkpoint,\n",
        "        steps_per_checkpoint=steps_per_checkpoint,\n",
        "        num_cases=val_size,\n",
        "        plot_fn=os.path.join(log_folder,\"progress\"),\n",
        "        log_wandb=False,\n",
        "\n",
        "    )\n",
        "    # Save model weights each checkpoint\n",
        "    gen_weights_file = os.path.join(\n",
        "        model_weights_root, f\"gen_weights-{training_samples:07d}.h5\"\n",
        "    )\n",
        "    model.gen.save_weights(gen_weights_file)\n",
        "\n",
        "    training_samples += steps_per_checkpoint * batch_size\n",
        "    checkpoint += 1\n",
        "\n",
        "print(\"Finished\")"
      ],
      "metadata": {
        "id": "QqxL0K0pamzw"
      },
      "id": "QqxL0K0pamzw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import os\n",
        "# Assurez-vous d'avoir installé tqdm: pip install tqdm\n",
        "from tqdm import tqdm\n",
        "from dsrnngan.model.train import train_model\n",
        "\n",
        "num_samples = 6\n",
        "steps_per_checkpoint = 3\n",
        "batch_size = 2\n",
        "# Le calcul donne 1 seul checkpoint : 6 / (3 * 2) = 1\n",
        "num_checkpoints = int(num_samples / (steps_per_checkpoint * batch_size))\n",
        "checkpoint = 1  # Corrigé à 1 pour la logique d'affichage\n",
        "\n",
        "print(\"Doing a training across\", num_checkpoints,\"checkpoints, all log results will be stored in,\", log_folder)\n",
        "if not os.path.exists(log_folder+\"models/\"):\n",
        "    os.makedirs(log_folder+\"models/\")\n",
        "\n",
        "model_weights_root = os.path.join(log_folder, \"models\")\n",
        "print(model_weights_root )\n",
        "\n",
        "training_samples = 0\n",
        "\n",
        "# Utilisation de tqdm pour visualiser la progression sur tous les checkpoints\n",
        "# La boucle s'exécute tant que le nombre d'échantillons entraînés est inférieur au total.\n",
        "with tqdm(total=num_checkpoints, desc=\"Training Checkpoints\") as pbar:\n",
        "    while training_samples < num_samples:  # main training loop\n",
        "        gc.collect()\n",
        "\n",
        "        # Affichage du statut du checkpoint\n",
        "        pbar.set_description(f\"Checkpoint {checkpoint}/{num_checkpoints}\")\n",
        "\n",
        "        # --- Bloc d'entraînement ---\n",
        "        loss_log = train_model(\n",
        "            model=model,\n",
        "            mode=mode,\n",
        "            batch_gen_train=batch_gen_train,\n",
        "            data_gen_valid=batch_gen_valid ,\n",
        "            noise_channels=noise_channels,\n",
        "            latent_variables=latent_variables,\n",
        "            checkpoint=checkpoint,\n",
        "            steps_per_checkpoint=steps_per_checkpoint,\n",
        "            num_cases=val_size,\n",
        "            plot_fn=os.path.join(log_folder,\"progress\"),\n",
        "            log_wandb=False,\n",
        "        )\n",
        "        # --- Fin Bloc d'entraînement ---\n",
        "\n",
        "        # Sauvegarde des poids\n",
        "        gen_weights_file = os.path.join(\n",
        "            model_weights_root, f\"gen_weights-{training_samples:07d}.h5\"\n",
        "        )\n",
        "        # model.gen.save_weights(gen_weights_file) # Décommenter cette ligne dans le vrai code\n",
        "\n",
        "        # Mise à jour des compteurs et de la barre de progression\n",
        "        training_samples += steps_per_checkpoint * batch_size\n",
        "        checkpoint += 1\n",
        "        pbar.update(1)\n",
        "\n",
        "print(\"Finished\")"
      ],
      "metadata": {
        "id": "gqaw3uxu1CK3"
      },
      "id": "gqaw3uxu1CK3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now check that the evaluation is working"
      ],
      "metadata": {
        "id": "h6Bw6z9Taqfm"
      },
      "id": "h6Bw6z9Taqfm"
    },
    {
      "cell_type": "code",
      "source": [
        "eval_fname = os.path.join(log_folder, \"eval_validation.txt\")\n",
        "print(\"Generating evaluation report to be stored in\", eval_fname)\n",
        "\n",
        "evaluate_multiple_checkpoints(\n",
        "            mode=mode,\n",
        "            arch=arch,\n",
        "            val_years=val_years,\n",
        "            log_fname=eval_fname,\n",
        "            weights_dir=model_weights_root,\n",
        "            autocoarsen=False,\n",
        "            add_noise=add_noise,\n",
        "            noise_factor=noise_factor,\n",
        "            model_numbers=[0],\n",
        "            ranks_to_save=[],\n",
        "            num_images=num_images,\n",
        "            filters_gen=filters_gen,\n",
        "            filters_disc=filters_disc,\n",
        "            input_channels=input_channels,\n",
        "            constant_fields=constant_fields,\n",
        "            latent_variables=latent_variables,\n",
        "            noise_channels=noise_channels,\n",
        "            padding=padding,\n",
        "            ensemble_size=10,\n",
        "        )\n",
        "print(\"Finished\")"
      ],
      "metadata": {
        "id": "sdp7wcPUap4g"
      },
      "id": "sdp7wcPUap4g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now you can go check the evaluation results under the eval_validation.txt file in you log folder. For further training of the cGAN using the command line options is recommended. See the README under [model/](https://github.com/snath-xoc/cGAN_tutorial/tree/main/model) for more information\n"
      ],
      "metadata": {
        "id": "qau8MLdDavet"
      },
      "id": "qau8MLdDavet"
    },
    {
      "cell_type": "code",
      "source": [
        "input_image_shape = (data_config.input_image_height, data_config.input_image_width, data_config.input_channels) #352, 384, 4*n_variables (or the other way round 384, 352)\n",
        "output_image_shape = (model_config.downscaling_factor * input_image_shape[0], model_config.downscaling_factor * input_image_shape[1], 1)#352, 384 (or the other way round 384, 352)\n",
        "\n"
      ],
      "metadata": {
        "id": "GEm32YSj6A3f"
      },
      "id": "GEm32YSj6A3f",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "coiled",
      "language": "python",
      "name": "coiled"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "name": "Melvin_train_cgan_colab.ipynb",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}