{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "HOME = Path(os.getcwd()).parents[0]\n",
    "\n",
    "sys.path.insert(1, str(HOME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsrnngan.data import VAR_LOOKUP_ERA5, load_hdf5_file, filter_by_lat_lon, get_ifs_filepath, get_era5_filepath_prefix, get_era5_path, load_imerg_raw, get_era5_stats, filter_by_lat_lon, load_imerg_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read some sample data in \n",
    "# - Check units\n",
    "# - check orientation of lat and long\n",
    "# - Check times\n",
    "\n",
    "# Definitions are given here: https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "tp appears to be the total rainfall on a given day, in metres. NOTE: this is on a 0.25 degree grid (the rest are on a 1 degree grid)\n",
    "https://apps.ecmwf.int/codes/grib/param-db?id=228\n",
    "\n",
    "shum: I can't find in any of the definition urls, but it does have q as a variable which aligns\n",
    "https://apps.ecmwf.int/codes/grib/param-db?id=133\n",
    "kg kg-1 (mass of water vapour per kg of moist air)\n",
    "\n",
    "ta: Near surface air temperature . Presumably Kelvin?\n",
    "https://clipc-services.ceda.ac.uk/dreq/u/bab9237c-e5dd-11e5-8482-ac72891c3257.html\n",
    "\n",
    "u: u component of wind (eastward)\n",
    "https://apps.ecmwf.int/codes/grib/param-db?id=131\n",
    "\n",
    "v: v component of wind (northward)\n",
    "https://apps.ecmwf.int/codes/grib/param-db?id=132\n",
    "\n",
    "I think time_bnds is time bounds, presumably time bounds of data gathering? Do we need to check that some days don't have timny time windows?\n",
    "\n",
    "Rough corners of the box we are considering (1000km x 1000km)\n",
    "SE: -2.928111255616329, 43.25853873313056\n",
    "NE: 6.307770957731585, 43.25853873313056\n",
    "SW: -2.928111255616329, 33.80008363706967\n",
    "NW: 6.307770957731585, 33.80008363706967\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 'tp'\n",
    "year = 2020\n",
    "month = 11\n",
    "latitude_vals = list(range(-3, 8))\n",
    "longitude_vals = list(range(33, 44))\n",
    "log_precip = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: b'/user/home/uz22147/repos/downscaling-cgan/system_tests/data/ERA5/total_precipitation/ERA5_total_precipitation_day_202011.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/user/work/uz22147/miniconda3/lib/python3.9/site-packages/xarray/backends/file_manager.py:201\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     file \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cache[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_key]\n\u001b[1;32m    202\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m/user/work/uz22147/miniconda3/lib/python3.9/site-packages/xarray/backends/lru_cache.py:55\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m---> 55\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cache[key]\n\u001b[1;32m     56\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache\u001b[39m.\u001b[39mmove_to_end(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('/user/home/uz22147/repos/downscaling-cgan/system_tests/data/ERA5/total_precipitation/ERA5_total_precipitation_day_202011.nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False))]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/user/home/uz22147/repos/downscaling-cgan/notebooks/sample_data.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbp1-login/user/home/uz22147/repos/downscaling-cgan/notebooks/sample_data.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m data_folder \u001b[39m=\u001b[39m HOME \u001b[39m/\u001b[39m \u001b[39m'\u001b[39m\u001b[39msystem_tests\u001b[39m\u001b[39m'\u001b[39m \u001b[39m/\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbp1-login/user/home/uz22147/repos/downscaling-cgan/notebooks/sample_data.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m era5_path \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(data_folder \u001b[39m/\u001b[39m \u001b[39m'\u001b[39m\u001b[39mERA5\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bbp1-login/user/home/uz22147/repos/downscaling-cgan/notebooks/sample_data.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m ds \u001b[39m=\u001b[39m xr\u001b[39m.\u001b[39;49mload_dataset(get_era5_path(variable, year\u001b[39m=\u001b[39;49myear, month\u001b[39m=\u001b[39;49mmonth, era_data_dir\u001b[39m=\u001b[39;49mera5_path))\n",
      "File \u001b[0;32m/user/work/uz22147/miniconda3/lib/python3.9/site-packages/xarray/backends/api.py:270\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(filename_or_obj, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcache\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs:\n\u001b[1;32m    268\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcache has no effect in this context\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 270\u001b[0m \u001b[39mwith\u001b[39;00m open_dataset(filename_or_obj, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mas\u001b[39;00m ds:\n\u001b[1;32m    271\u001b[0m     \u001b[39mreturn\u001b[39;00m ds\u001b[39m.\u001b[39mload()\n",
      "File \u001b[0;32m/user/work/uz22147/miniconda3/lib/python3.9/site-packages/xarray/backends/api.py:531\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    519\u001b[0m decoders \u001b[39m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    520\u001b[0m     decode_cf,\n\u001b[1;32m    521\u001b[0m     open_backend_dataset_parameters\u001b[39m=\u001b[39mbackend\u001b[39m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m     decode_coords\u001b[39m=\u001b[39mdecode_coords,\n\u001b[1;32m    528\u001b[0m )\n\u001b[1;32m    530\u001b[0m overwrite_encoded_chunks \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39moverwrite_encoded_chunks\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 531\u001b[0m backend_ds \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39;49mopen_dataset(\n\u001b[1;32m    532\u001b[0m     filename_or_obj,\n\u001b[1;32m    533\u001b[0m     drop_variables\u001b[39m=\u001b[39;49mdrop_variables,\n\u001b[1;32m    534\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdecoders,\n\u001b[1;32m    535\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    536\u001b[0m )\n\u001b[1;32m    537\u001b[0m ds \u001b[39m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    538\u001b[0m     backend_ds,\n\u001b[1;32m    539\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    548\u001b[0m )\n\u001b[1;32m    549\u001b[0m \u001b[39mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/user/work/uz22147/miniconda3/lib/python3.9/site-packages/xarray/backends/netCDF4_.py:555\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, lock, autoclose)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopen_dataset\u001b[39m(\n\u001b[1;32m    535\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    536\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    551\u001b[0m     autoclose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    552\u001b[0m ):\n\u001b[1;32m    554\u001b[0m     filename_or_obj \u001b[39m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[0;32m--> 555\u001b[0m     store \u001b[39m=\u001b[39m NetCDF4DataStore\u001b[39m.\u001b[39;49mopen(\n\u001b[1;32m    556\u001b[0m         filename_or_obj,\n\u001b[1;32m    557\u001b[0m         mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m    558\u001b[0m         \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mformat\u001b[39;49m,\n\u001b[1;32m    559\u001b[0m         group\u001b[39m=\u001b[39;49mgroup,\n\u001b[1;32m    560\u001b[0m         clobber\u001b[39m=\u001b[39;49mclobber,\n\u001b[1;32m    561\u001b[0m         diskless\u001b[39m=\u001b[39;49mdiskless,\n\u001b[1;32m    562\u001b[0m         persist\u001b[39m=\u001b[39;49mpersist,\n\u001b[1;32m    563\u001b[0m         lock\u001b[39m=\u001b[39;49mlock,\n\u001b[1;32m    564\u001b[0m         autoclose\u001b[39m=\u001b[39;49mautoclose,\n\u001b[1;32m    565\u001b[0m     )\n\u001b[1;32m    567\u001b[0m     store_entrypoint \u001b[39m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    568\u001b[0m     \u001b[39mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[0;32m/user/work/uz22147/miniconda3/lib/python3.9/site-packages/xarray/backends/netCDF4_.py:384\u001b[0m, in \u001b[0;36mNetCDF4DataStore.open\u001b[0;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[1;32m    378\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m    379\u001b[0m     clobber\u001b[39m=\u001b[39mclobber, diskless\u001b[39m=\u001b[39mdiskless, persist\u001b[39m=\u001b[39mpersist, \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39mformat\u001b[39m\n\u001b[1;32m    380\u001b[0m )\n\u001b[1;32m    381\u001b[0m manager \u001b[39m=\u001b[39m CachingFileManager(\n\u001b[1;32m    382\u001b[0m     netCDF4\u001b[39m.\u001b[39mDataset, filename, mode\u001b[39m=\u001b[39mmode, kwargs\u001b[39m=\u001b[39mkwargs\n\u001b[1;32m    383\u001b[0m )\n\u001b[0;32m--> 384\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(manager, group\u001b[39m=\u001b[39;49mgroup, mode\u001b[39m=\u001b[39;49mmode, lock\u001b[39m=\u001b[39;49mlock, autoclose\u001b[39m=\u001b[39;49mautoclose)\n",
      "File \u001b[0;32m/user/work/uz22147/miniconda3/lib/python3.9/site-packages/xarray/backends/netCDF4_.py:332\u001b[0m, in \u001b[0;36mNetCDF4DataStore.__init__\u001b[0;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_group \u001b[39m=\u001b[39m group\n\u001b[1;32m    331\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mode \u001b[39m=\u001b[39m mode\n\u001b[0;32m--> 332\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mds\u001b[39m.\u001b[39mdata_model\n\u001b[1;32m    333\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_filename \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mds\u001b[39m.\u001b[39mfilepath()\n\u001b[1;32m    334\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_remote \u001b[39m=\u001b[39m is_remote_uri(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_filename)\n",
      "File \u001b[0;32m/user/work/uz22147/miniconda3/lib/python3.9/site-packages/xarray/backends/netCDF4_.py:393\u001b[0m, in \u001b[0;36mNetCDF4DataStore.ds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    392\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mds\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 393\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_acquire()\n",
      "File \u001b[0;32m/user/work/uz22147/miniconda3/lib/python3.9/site-packages/xarray/backends/netCDF4_.py:387\u001b[0m, in \u001b[0;36mNetCDF4DataStore._acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_acquire\u001b[39m(\u001b[39mself\u001b[39m, needs_lock\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 387\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_manager\u001b[39m.\u001b[39macquire_context(needs_lock) \u001b[39mas\u001b[39;00m root:\n\u001b[1;32m    388\u001b[0m         ds \u001b[39m=\u001b[39m _nc4_require_group(root, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_group, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mode)\n\u001b[1;32m    389\u001b[0m     \u001b[39mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/user/work/uz22147/miniconda3/lib/python3.9/contextlib.py:119\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[1;32m    118\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[1;32m    120\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m/user/work/uz22147/miniconda3/lib/python3.9/site-packages/xarray/backends/file_manager.py:189\u001b[0m, in \u001b[0;36mCachingFileManager.acquire_context\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39m@contextlib\u001b[39m\u001b[39m.\u001b[39mcontextmanager\n\u001b[1;32m    187\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39macquire_context\u001b[39m(\u001b[39mself\u001b[39m, needs_lock\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    188\u001b[0m     \u001b[39m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m     file, cached \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_acquire_with_cache_info(needs_lock)\n\u001b[1;32m    190\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m         \u001b[39myield\u001b[39;00m file\n",
      "File \u001b[0;32m/user/work/uz22147/miniconda3/lib/python3.9/site-packages/xarray/backends/file_manager.py:207\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    205\u001b[0m     kwargs \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    206\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mmode\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mode\n\u001b[0;32m--> 207\u001b[0m file \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_opener(\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    208\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    209\u001b[0m     \u001b[39m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2463\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2026\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: b'/user/home/uz22147/repos/downscaling-cgan/system_tests/data/ERA5/total_precipitation/ERA5_total_precipitation_day_202011.nc'"
     ]
    }
   ],
   "source": [
    "year = 2020\n",
    "month = 11\n",
    "day = 1\n",
    "data_folder = HOME / 'system_tests' / 'data'\n",
    "era5_path = str(data_folder / 'ERA5')\n",
    "ds = xr.load_dataset(get_era5_path(variable, year=year, month=month, era_data_dir=era5_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get small subset of ERA5 data\n",
    "\n",
    "for v in VAR_LOOKUP_ERA5:\n",
    "    input_year = 2018\n",
    "    for input_month in range(1,13):\n",
    "\n",
    "        latitude_vals=[-2, -1, 0 , 1, 2]\n",
    "        longitude_vals=[32, 33, 34, 35]\n",
    "        ds = xr.load_dataset(get_era5_path(v, year=input_year, month=input_month))\n",
    "\n",
    "        lon_var_name = 'longitude' if v == 'tp' else 'lon'\n",
    "        lat_var_name = 'latitude' if v == 'tp' else 'lat'\n",
    "\n",
    "        ds = filter_by_lat_lon(ds, lon_range=longitude_vals, \n",
    "                                   lat_range=latitude_vals,\n",
    "                                   lon_var_name=lon_var_name,\n",
    "                                   lat_var_name=lat_var_name)\n",
    "\n",
    "        fp = get_era5_path(variable=v, year=input_year, month=input_month, era_data_dir=str(HOME / 'system_tests/data/ERA5'))\n",
    "        ds.to_netcdf(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../system_tests/data/IMERG/half_hourly/final/3B-HHR.MS.MRG.3IMERG.20181230-S183000-E185959.1110.V06B.nc\n",
      "../system_tests/data/IMERG/half_hourly/final/3B-HHR.MS.MRG.3IMERG.20181230-S173000-E175959.1050.V06B.nc\n",
      "../system_tests/data/IMERG/half_hourly/final/3B-HHR.MS.MRG.3IMERG.20170704-S183000-E185959.1110.V06B.nc\n",
      "../system_tests/data/IMERG/half_hourly/final/3B-HHR.MS.MRG.3IMERG.20181230-S170000-E172959.1020.V06B.nc\n",
      "../system_tests/data/IMERG/half_hourly/final/3B-HHR.MS.MRG.3IMERG.20170704-S173000-E175959.1050.V06B.nc\n",
      "../system_tests/data/IMERG/half_hourly/final/3B-HHR.MS.MRG.3IMERG.20170704-S170000-E172959.1020.V06B.nc\n",
      "../system_tests/data/IMERG/half_hourly/final/3B-HHR.MS.MRG.3IMERG.20181231-S170000-E172959.1020.V06B.nc\n",
      "../system_tests/data/IMERG/half_hourly/final/3B-HHR.MS.MRG.3IMERG.20181230-S180000-E182959.1080.V06B.nc\n",
      "../system_tests/data/IMERG/half_hourly/final/3B-HHR.MS.MRG.3IMERG.20170704-S180000-E182959.1080.V06B.nc\n",
      "../system_tests/data/IMERG/half_hourly/final/3B-HHR.MS.MRG.3IMERG.20181231-S173000-E175959.1050.V06B.nc\n",
      "../system_tests/data/IMERG/half_hourly/final/3B-HHR.MS.MRG.3IMERG.20181231-S180000-E182959.1080.V06B.nc\n",
      "../system_tests/data/IMERG/half_hourly/final/3B-HHR.MS.MRG.3IMERG.20181231-S183000-E185959.1110.V06B.nc\n"
     ]
    }
   ],
   "source": [
    "# Get small sample of imerg data\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "latitude_vals=[-2, -1, 0 , 1, 2]\n",
    "longitude_vals=[32, 33, 34, 35]\n",
    "imerg_data_dir = '/bp1/geog-tropical/data/Obs/IMERG/half_hourly/final/'\n",
    "glob_str = os.path.join(imerg_data_dir, '3B-HHR.MS.MRG.3IMERG.2018123*-S18*')\n",
    "glob_str_2 = os.path.join(imerg_data_dir, '3B-HHR.MS.MRG.3IMERG.20170704*-S18*')\n",
    "glob_str_3 = os.path.join(imerg_data_dir, '3B-HHR.MS.MRG.3IMERG.2018123*-S17*')\n",
    "glob_str_4 = os.path.join(imerg_data_dir, '3B-HHR.MS.MRG.3IMERG.20170704*-S17*')\n",
    "\n",
    "fps = set(glob(glob_str) + glob(glob_str_2) + glob(glob_str_3) + glob(glob_str_4))\n",
    "\n",
    "for fp in fps:\n",
    "    output_imerg_path = os.path.join('../system_tests/data/IMERG/half_hourly/final', fp.split('/')[-1].replace('HDF5', 'nc'))\n",
    "    print(output_imerg_path)\n",
    "    ds = load_hdf5_file(fp)\n",
    "    small_ds = filter_by_lat_lon(ds, lon_range=longitude_vals, \n",
    "                                   lat_range=latitude_vals,\n",
    "                                   lon_var_name='lon',\n",
    "                                   lat_var_name='lat')\n",
    "    \n",
    "    small_ds.to_netcdf(output_imerg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2018\n",
    "month = 12\n",
    "day = 30\n",
    "hour = 18\n",
    "latitude_vals = [0, 0.1, 0.2, 0.3, 0.4]\n",
    "longitude_vals = [33, 34]\n",
    "\n",
    "ds = load_imerg_raw(year=year, month=month, day=day, hour=hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05 0.15 0.25 0.35 0.45]\n"
     ]
    }
   ],
   "source": [
    "# original_lat_vals = ds.lat.values\n",
    "# new_lat_vals = ds.lat.values + 1e-6\n",
    "# ds.reindex(lat=list(set(new_lat_vals)), lon=list(set(ds.lon.values)), latv=list(set(ds.latv.values)), lonv=list(set(ds.lonv.values)))\n",
    "ds = ds.sel(lat=latitude_vals, method='backfill')\n",
    "print(ds.lat.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/bp1/geog-tropical/users/uz22147/east_africa_data/IFS/tp/tp_HRES_1h_EAfrica_2017-07-04_*h.nc'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ifs_filepath('tp', loaddate=datetime(2017, 7, 4), loadtime='*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get a sample of IFS data\n",
    "import glob\n",
    "from calendar import monthrange\n",
    "\n",
    "from dsrnngan.data import all_ifs_fields, get_ifs_filepath\n",
    "latitude_vals=[-2, -1, 0 , 1, 2]\n",
    "longitude_vals=[32, 33, 34, 35]\n",
    "\n",
    "year = 2017\n",
    "month = 7\n",
    "\n",
    "for day in [4, 5]:\n",
    "    for var in all_ifs_fields:\n",
    "        glob_str = get_ifs_filepath(var, loaddate=datetime(year, month ,day), loadtime='*')\n",
    "        fps = glob.glob(glob_str)\n",
    "        for fp in fps:\n",
    "            suffix = fp.split('/')[-1]\n",
    "            loadtime = int(suffix.split('.')[0].split('_')[-1].replace('h', ''))\n",
    "            ds = xr.open_dataset(fp)\n",
    "            small_ds = filter_by_lat_lon(ds, lon_range=longitude_vals, lat_range=latitude_vals)\n",
    "            output_fp = get_ifs_filepath(var, loaddate=datetime(year, month, day), loadtime=loadtime, fcst_dir='../system_tests/data/IFS/')\n",
    "            small_ds.to_netcdf(output_fp)\n",
    "\n",
    "\n",
    "                # vars = [fp.split('/')[-1].split('_')[0] for fp in fps]\n",
    "                # for n, var in enumerate(vars):\n",
    "\n",
    "                #     fp = [fp for fp in fps if fp.split('/')[-1].startswith(var)][0]\n",
    "                #     suffix = fp.split('/')[-1]\n",
    "                #     ds = xr.open_dataset(fp)\n",
    "                #     var_name = list(ds.data_vars)[0]\n",
    "                    \n",
    "                #     small_ds = filter_by_lat_lon(ds, lon_range=longitude_vals, lat_range=latitude_vals)\n",
    "                #     sample = small_ds.sel(time=slice(f\"{year}-07-05T04\", \"2017-07-05T05\"))\n",
    "\n",
    "                #     sample.to_netcdf(f'../system_tests/data/IFS/{suffix}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "year = 2018\n",
    "month = 12\n",
    "day = 31\n",
    "\n",
    "glob.glob(f'/bp1/geog-tropical/users/uz22147/IFS_east_africa/tp/*_HRES_1h_EAfrica_{year}-{month:02d}-{day:02d}*.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.load_dataset('/user/home/uz22147/repos/downscaling-cgan/system_tests/data/IFS/r/r200/r200_HRES_1h_EAfrica_2017-07-04_12h.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([item for item in ds['r'].values.flatten() if item < 0]) / len( ds['r'].values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sample of nimrod data\n",
    "path_2018 = '/user/home/uz22147/repos/downscaling-cgan/system_tests/data/NIMROD/2018/metoffice-c-band-rain-radar_uk_20181231.nc'\n",
    "path_2019 = '/user/home/uz22147/repos/downscaling-cgan/system_tests/data/NIMROD/2019/metoffice-c-band-rain-radar_uk_20191231.nc'\n",
    "\n",
    "ds_2018 = xr.open_dataset(path_2018)\n",
    "# .sel(latitude=[-1, 0,1, 2], longitude=[32, 33, 34, 35], method='nearest')\n",
    "ds_2019 = xr.open_dataset(path_2019)\n",
    "# .sel(latitude=[-1, 0,1, 2], longitude=[32, 33, 34, 35], method='nearest')\n",
    "\n",
    "# ds_2018.to_netcdf('/user/home/uz22147/repos/downscaling-cgan/system_tests/data/NIMROD/2018/metoffice-c-band-rain-radar_uk_20181231.nc')\n",
    "# ds_2019.to_netcdf('/user/home/uz22147/repos/downscaling-cgan/system_tests/data/NIMROD/2019/metoffice-c-band-rain-radar_uk_20191231.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample of constants\n",
    "latitude_vals=[-2, -1, 0 , 1, 2]\n",
    "longitude_vals=[32, 33, 34, 35]\n",
    "\n",
    "h_ds = xr.open_dataset('/bp1/geog-tropical/users/uz22147/east_africa_data/constants/h_HRES_EAfrica.nc')\n",
    "lsm_ds = xr.open_dataset('/bp1/geog-tropical/users/uz22147/east_africa_data/constants/lsm_HRES_EAfrica.nc')\n",
    "\n",
    "h_ds = filter_by_lat_lon(h_ds, lon_range=longitude_vals, lat_range=latitude_vals)\n",
    "lsm_ds = filter_by_lat_lon(lsm_ds, lon_range=longitude_vals, lat_range=latitude_vals)\n",
    "\n",
    "h_ds.to_netcdf(f'../system_tests/data/constants/h_HRES_EAfrica.nc')\n",
    "lsm_ds.to_netcdf(f'../system_tests/data/constants/lsm_HRES_EAfrica.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder = \"../system_tests/data/tfrecords/IFS_nimrod/\"\n",
    "\n",
    "fle_hdles = []\n",
    "year = 2018\n",
    "hour = 9\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample of tfrecords\n",
    "import tensorflow as tf\n",
    "\n",
    "for fh in range(4):\n",
    "    flename = os.path.join(f\"../system_tests/data/tfrecords/IFS_nimrod/\", 'samples', f\"2018_9.{fh}.tfrecords\")\n",
    "    file_handle = tf.io.TFRecordWriter(flename)\n",
    "    \n",
    "    raw_dataset = tf.data.TFRecordDataset(f\"../system_tests/data/tfrecords/IFS_nimrod/2018_9.{fh}.tfrecords\")\n",
    "    \n",
    "    for raw_record in raw_dataset.take(100):\n",
    "        example = tf.train.Example()\n",
    "        example.ParseFromString(raw_record.numpy())\n",
    "        \n",
    "        example_to_string = example.SerializeToString()\n",
    "        file_handle.write(example_to_string)\n",
    "\n",
    "    file_handle.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set\n",
    "flename = os.path.join(f\"../system_tests/data/tfrecords/IFS_nimrod/\", 'samples', f\"validation2019.tfrecords\")\n",
    "\n",
    "file_handle = tf.io.TFRecordWriter(flename)\n",
    "raw_dataset = tf.data.TFRecordDataset(f\"../system_tests/data/tfrecords/IFS_nimrod/validation2019.tfrecords\")\n",
    "\n",
    "for raw_record in raw_dataset.take(100):\n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(raw_record.numpy())\n",
    "    \n",
    "    example_to_string = example.SerializeToString()\n",
    "    file_handle.write(example_to_string)\n",
    "\n",
    "file_handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Same for ERA5 / iMERG\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "for fh in range(4):\n",
    "        \n",
    "    raw_dataset = tf.data.TFRecordDataset(f\"/user/work/uz22147/tfrecords/era5_imerg/2018_9.{fh}.tfrecords\")\n",
    "    \n",
    "    flename = os.path.join(f\"../system_tests/data/tfrecords/ERA5_iMERG/\", f\"2018_9.{fh}.tfrecords\")\n",
    "    file_handle = tf.io.TFRecordWriter(flename)\n",
    "\n",
    "    m = 0\n",
    "    for raw_record in raw_dataset.take(1000):\n",
    "        print(m)\n",
    "        m+=1\n",
    "        example = tf.train.Example()\n",
    "        example.ParseFromString(raw_record.numpy())\n",
    "        \n",
    "        example_to_string = example.SerializeToString()\n",
    "        file_handle.write(example_to_string)\n",
    "\n",
    "    file_handle.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "fl = glob(f\"/user/home/uz22147/repos/downscaling-cgan/data/tfrecords/2018_9.0.tfrecords\")\n",
    "\n",
    "ds = tf.data.TFRecordDataset(fl)\n",
    "insize=(20, 20, 9)\n",
    "consize=(200, 200, 2)\n",
    "outsize=(200, 200, 1)\n",
    "    # Create a description of the features\n",
    "feature_description = {\n",
    "    'generator_input': tf.io.FixedLenFeature(insize, tf.float32),\n",
    "    'constants': tf.io.FixedLenFeature(consize, tf.float32),\n",
    "    'generator_output': tf.io.FixedLenFeature(outsize, tf.float32),\n",
    "}\n",
    "for raw_record in ds.take(100):\n",
    "    example = tf.io.parse_example(raw_record, feature_description)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e41783afaa3f5d449c229f6d97ffb609c8a0e066baa45e2dd73a3d3295527dac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
